# Google Gemini API Configuration
GEMINI_API_KEY=your_gemini_api_key_here
# Default model: gemini-3-flash-preview (fast, cost-effective)
# For best results with complex recipes, use: gemini-3-pro-preview
GEMINI_MODEL=gemini-3-flash-preview

# Image Detection Model Configuration (separate from main recipe model)
# Default model: gemini-2.5-flash-lite (fast, cost-effective for vision tasks)
# For better accuracy with complex images, use: gemini-3-pro-preview
# This allows using different models for recipe recommendations vs ingredient detection
IMAGE_DETECTION_MODEL=gemini-2.5-flash-lite

# Memory Model Configuration (separate from main recipe model)
# Default model: gemini-2.5-flash-lite (cost-optimized for memory operations)
# Used for user memories, session summaries, and tool result compression
# Can be different from main model for cost optimization
MEMORY_MODEL=gemini-2.5-flash-lite

# LLM Model Parameters
# Temperature: Controls randomness in responses (0.0 = deterministic, 1.0 = maximum randomness)
# For recipes: 0.2 balances creativity with consistency in recipe suggestions
TEMPERATURE=0.2

# Max Output Tokens: Maximum length of model response
# For recipes with multiple recommendations: 8192 supports full response with 10 recipes
# Gemini 3 Flash supports up to 65,536 output tokens
MAX_OUTPUT_TOKENS=8192

# Thinking Level: Extended reasoning capability for complex tasks
# Options: "low" (balanced), "high" (most thorough)
# Recipe recommendations work well with None
THINKING_LEVEL=

# Spoonacular Recipe API Configuration
# Enable/disable Spoonacular MCP (external recipe API)
# When enabled: Uses Spoonacular for recipe search and details
# When disabled: Uses internal LLM knowledge to generate recipes
USE_SPOONACULAR=true
SPOONACULAR_API_KEY=your_spoonacular_api_key_here

# Server Configuration
PORT=7777

# Agent Memory Configuration
MAX_HISTORY=3

# Recipe Search Configuration
MAX_RECIPES=3

# Image Processing Configuration
MAX_IMAGE_SIZE_MB=5
MIN_INGREDIENT_CONFIDENCE=0.7

# Image Compression Configuration
# Only compress images smaller than this threshold (in KB)
# Images above this size are already compressed enough
# Default: 300 KB - balances compression benefit vs processing overhead
# Set to 0 to disable compression, or increase for more aggressive compression
COMPRESS_IMG_THRESHOLD_KB=300

# Tool Call Limit: Maximum number of tool calls agent can make per request
# This prevents excessive API calls and helps control costs
TOOL_CALL_LIMIT=12

# Ingredient Detection Mode: "pre-hook" (default) or "tool"
# - pre-hook: Extract ingredients before agent processes (faster, no extra LLM call)
# - tool: Register ingredient detection as agent @tool (agent has full control)
IMAGE_DETECTION_MODE=pre-hook

# Logging Configuration
LOG_LEVEL=INFO
LOG_TYPE=text

# Database Configuration (Optional - uses SQLite if not set)
# DATABASE_URL=postgresql://user:password@localhost:5432/recipe_service

# Tracing Configuration
# Enable tracing for observability (requires OpenTelemetry packages)
ENABLE_TRACING=true

# Tracing database type: "sqlite" or "postgres"
TRACING_DB_TYPE=sqlite

# Path for SQLite tracing database (ignored if using PostgreSQL)
TRACING_DB_FILE=agno_traces.db

# ===== Agent Retry Configuration =====
# Handles transient failures with exponential backoff

# MAX_RETRIES: Number of retry attempts for failed API calls
# Example: 3 retries with 2s initial delay = 2s → 4s → 8s delays (exponential backoff)
MAX_RETRIES=3

# DELAY_BETWEEN_RETRIES: Initial delay in seconds (doubled each retry if exponential backoff enabled)
DELAY_BETWEEN_RETRIES=2

# EXPONENTIAL_BACKOFF: Enable exponential backoff for rate limit handling
# When enabled: delays double each retry (handles rate limiting gracefully)
EXPONENTIAL_BACKOFF=true

# ===== Agent Memory & History Settings =====
# Control context enrichment and knowledge management

# ADD_HISTORY_TO_CONTEXT: Include conversation history in LLM context for coherence
# Helps agent maintain continuity across multi-turn conversations
ADD_HISTORY_TO_CONTEXT=true

# READ_TOOL_CALL_HISTORY: Give LLM access to previous tool calls
# Allows agent to see what tools were called before and their results
READ_TOOL_CALL_HISTORY=false

# UPDATE_KNOWLEDGE: Allow LLM to add learnings to knowledge base
# Agent can store findings for future reference and improved recommendations
UPDATE_KNOWLEDGE=true

# READ_CHAT_HISTORY: Provide dedicated tool for LLM to query chat history
# LLM can explicitly search conversation history when needed
READ_CHAT_HISTORY=false

# ENABLE_USER_MEMORIES: Store and track user preferences for personalization
# Preferences (diet, cuisine, intolerances) persisted per session
ENABLE_USER_MEMORIES=true

# ENABLE_SESSION_SUMMARIES: Auto-summarize sessions for context compression
# Helps manage token usage while maintaining context for long conversations
ENABLE_SESSION_SUMMARIES=false

# COMPRESS_TOOL_RESULTS: Compress tool outputs to reduce context size
# Removes redundant information from tool outputs to save tokens
COMPRESS_TOOL_RESULTS=true

# SEARCH_KNOWLEDGE: Give LLM ability to search knowledge base during reasoning
# Allows agent to check past learnings before making new API calls
SEARCH_KNOWLEDGE=true
