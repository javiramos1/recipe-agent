# Google Gemini API Configuration
GEMINI_API_KEY=your_gemini_api_key_here
# Default model: gemini-3-flash-preview (fast, cost-effective)
# For best results with complex recipes, use: gemini-3-pro-preview
GEMINI_MODEL=gemini-3-flash-preview

# Image Detection Model Configuration (separate from main recipe model)
# Default model: gemini-2.5-flash-lite (fast, cost-effective for vision tasks)
# For better accuracy with complex images, use: gemini-3-pro-preview
# This allows using different models for recipe recommendations vs ingredient detection
IMAGE_DETECTION_MODEL=gemini-2.5-flash-lite

# Agent Management Model Configuration (separate from main recipe model)
# Default model: gemini-2.5-flash-lite (cost-optimized for background operations)
# Used for user memories, session summaries, tool compression, and learning extraction
# Can be different from main model for cost optimization (98% cheaper)
AGENT_MNGT_MODEL=gemini-2.5-flash-lite

# LLM Model Parameters
# Temperature: Controls randomness in responses (0.0 = deterministic, 1.0 = maximum randomness)
# For recipes: 0.2 balances creativity with consistency in recipe suggestions
TEMPERATURE=0.2

# Max Output Tokens: Maximum length of model response
# For recipes with multiple recommendations: 8192 supports full response with 10 recipes
# Gemini 3 Flash supports up to 65,536 output tokens
MAX_OUTPUT_TOKENS=8192

# Thinking Level: Extended reasoning capability for complex tasks
# Options: "low" (balanced), "high" (most thorough)
# Recipe recommendations work well with None
THINKING_LEVEL=

# ===== Agent Context Awareness Configuration =====
# Enable temporal and location context for enhanced recommendations

# ADD_DATETIME_TO_CONTEXT: Include current date/time in agent reasoning
# Benefits: Time-aware recipes ("quick weeknight dinners", "summer recipes", seasonal ingredients)
# Default: true
ADD_DATETIME_TO_CONTEXT=true

# TIMEZONE_IDENTIFIER: Timezone for datetime context (TZ Database format)
# Examples: "Etc/UTC", "America/New_York", "Europe/London", "Asia/Tokyo"
# Default: "Etc/UTC"
TIMEZONE_IDENTIFIER=Etc/UTC

# ADD_LOCATION_TO_CONTEXT: Include user location in agent reasoning
# Benefits: Location-aware recipes (local ingredients, regional cuisines)
# Warning: Privacy consideration - requires user permission
# Default: false (disabled for privacy)
ADD_LOCATION_TO_CONTEXT=false

# Spoonacular Recipe API Configuration
# Enable/disable Spoonacular MCP (external recipe API)
# When enabled: Uses Spoonacular for recipe search and details
# When disabled: Uses internal LLM knowledge to generate recipes
USE_SPOONACULAR=true
SPOONACULAR_API_KEY=your_spoonacular_api_key_here

# Server Configuration
PORT=7777

# Agent Memory Configuration
MAX_HISTORY=3

# Recipe Search Configuration
MAX_RECIPES=3

# Image Processing Configuration
MAX_IMAGE_SIZE_MB=5
MIN_INGREDIENT_CONFIDENCE=0.7

# Image Compression Configuration
# Only compress images smaller than this threshold (in KB)
# Images above this size are already compressed enough
# Default: 300 KB - balances compression benefit vs processing overhead
# Set to 0 to disable compression, or increase for more aggressive compression
COMPRESS_IMG_THRESHOLD_KB=300

# Tool Call Limit: Maximum number of tool calls agent can make per request
# This prevents excessive API calls and helps control costs
TOOL_CALL_LIMIT=12

# Ingredient Detection Mode: "pre-hook" (default) or "tool"
# - pre-hook: Extract ingredients before agent processes (faster, no extra LLM call)
# - tool: Register ingredient detection as agent @tool (agent has full control)
IMAGE_DETECTION_MODE=pre-hook

# Logging Configuration
LOG_LEVEL=INFO
LOG_TYPE=text

# Database Configuration (Optional - uses SQLite if not set)
# DATABASE_URL=postgresql://user:password@localhost:5432/recipe_service

# Tracing Configuration
# Enable tracing for observability (requires OpenTelemetry packages)
ENABLE_TRACING=true

# Tracing database type: "sqlite" or "postgres"
TRACING_DB_TYPE=sqlite

# Path for SQLite tracing database (ignored if using PostgreSQL)
TRACING_DB_FILE=agno_traces.db

# ===== Agent Retry Configuration =====
# Handles transient failures with exponential backoff

# MAX_RETRIES: Number of retry attempts for failed API calls
# Example: 3 retries with 2s initial delay = 2s → 4s → 8s delays (exponential backoff)
MAX_RETRIES=3

# DELAY_BETWEEN_RETRIES: Initial delay in seconds (doubled each retry if exponential backoff enabled)
DELAY_BETWEEN_RETRIES=2

# EXPONENTIAL_BACKOFF: Enable exponential backoff for rate limit handling
# When enabled: delays double each retry (handles rate limiting gracefully)
EXPONENTIAL_BACKOFF=true

# ===== Agent Memory & History Settings =====
# Control context enrichment and knowledge management

# ADD_HISTORY_TO_CONTEXT: Include conversation history in LLM context for coherence
# Helps agent maintain continuity across multi-turn conversations
ADD_HISTORY_TO_CONTEXT=true

# READ_TOOL_CALL_HISTORY: Give LLM access to previous tool calls
# Allows agent to see what tools were called before and their results
READ_TOOL_CALL_HISTORY=false

# UPDATE_KNOWLEDGE: Allow LLM to add learnings to knowledge base
# Agent can store findings for future reference and improved recommendations
UPDATE_KNOWLEDGE=true

# READ_CHAT_HISTORY: Provide dedicated tool for LLM to query chat history
# LLM can explicitly search conversation history when needed
READ_CHAT_HISTORY=false

# ENABLE_USER_MEMORIES: Store and track user preferences for personalization
# Preferences (diet, cuisine, intolerances) persisted per session
ENABLE_USER_MEMORIES=true

# ENABLE_SESSION_SUMMARIES: Auto-summarize sessions for context compression
# Helps manage token usage while maintaining context for long conversations
ENABLE_SESSION_SUMMARIES=false

# COMPRESS_TOOL_RESULTS: Compress tool outputs to reduce context size
# Removes redundant information from tool outputs to save tokens
COMPRESS_TOOL_RESULTS=true

# SEARCH_KNOWLEDGE: Give LLM ability to search knowledge base during reasoning
# Allows agent to check past learnings before making new API calls
SEARCH_KNOWLEDGE=true

# SEARCH_SESSION_HISTORY: Search across multiple past sessions for recipe preferences
# Enables long-term preference tracking across conversation threads
SEARCH_SESSION_HISTORY=true

# NUM_HISTORY_SESSIONS: Number of past sessions to include in history search
# Recommended: 2-3 to balance long-term context with performance
# Note: Higher numbers increase context size and may slow requests
NUM_HISTORY_SESSIONS=2

# ===== Learning Machine Configuration =====
# Enables agent learning: dynamic profile extraction and insight storage

# ENABLE_LEARNING: Enable Learning Machine for agent learning and improvement
# When true: Agent extracts user profiles, preferences, and saves learned recipes
# When false: Stateless mode (no learning across sessions)
ENABLE_LEARNING=true

# LEARNING_MODE: How agent learns - "ALWAYS", "AGENTIC" (recommended), or "PROPOSE"
# ALWAYS: Automatic extraction after each response (extra LLM calls)
# AGENTIC: Agent receives tools, decides what to save (balanced, default)
# PROPOSE: Agent proposes learnings, user confirms before saving (high-stakes)
LEARNING_MODE=AGENTIC

# ADD_LEARNINGS_TO_CONTEXT: Automatically inject learned insights into context
# **CRITICAL: Set based on LEARNING_MODE:**
# - AGENTIC mode (recommended): Set to false
#   Agent controls learnings via search_learnings/save_learning tools
#   Auto-injection creates redundancy and token bloat
# - ALWAYS mode: Set to true
#   Relies on automatic extraction + context injection
# - PROPOSE mode: Set to true
#   Learnings need context for user review
# Default: false (aligns with recommended AGENTIC mode)
# Cost: No extra API calls, but increases context size if true
ADD_LEARNINGS_TO_CONTEXT=false

# ===== Knowledge vs LearnedKnowledge Note =====
# SEARCH_KNOWLEDGE: For external documents/FAQs (read-only, static data)
# ENABLE_LEARNING: For dynamic insights from conversations (agent-driven, dynamic)
# For recipes: Use LearnedKnowledge (dynamic) not Knowledge (static)
# WARNING: Using both is overkill and wastes resources - choose one

# ===== Agent Performance & Debugging Configuration =====
# Optimize performance and enable troubleshooting

# CACHE_SESSION: Cache agent session in memory for faster access
# Benefits: Faster subsequent requests for same session, reduced database queries
# Trade-offs: Increased memory usage; may have stale data in distributed systems
# Default: false (disabled for multi-server deployments)
CACHE_SESSION=false

# DEBUG_MODE: Enable debug logging and detailed agent output
# Benefits: Detailed logs for troubleshooting, see compiled system message
# Trade-offs: Verbose output, slower response times
# Default: false (disabled for production)
DEBUG_MODE=false
